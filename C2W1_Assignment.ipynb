{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuW-xg_bTsaF"
      },
      "source": [
        "# Week 1: Using CNN's with the Cats vs Dogs Dataset\n",
        "\n",
        "Welcome to the 1st assignment of the course! This week, you will be using the famous `Cats vs Dogs` dataset to train a model that can classify images of dogs from images of cats. For this, you will create your own Convolutional Neural Network in Tensorflow and leverage Keras' image preprocessing utilities.\n",
        "\n",
        "You will also create some helper functions to move the images around the filesystem so if you are not familiar with the `os` module be sure to take a look a the [docs](https://docs.python.org/3/library/os.html).\n",
        "\n",
        "Let's get started!"
      ],
      "id": "AuW-xg_bTsaF"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dn-6c02VmqiN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "dn-6c02VmqiN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLTQd84RUs1j"
      },
      "source": [
        "Download the dataset from its original source by running the cell below. \n",
        "\n",
        "Note that the `zip` file that contains the images is unzipped under the `/tmp` directory."
      ],
      "id": "bLTQd84RUs1j"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3sd9dQWa23aj",
        "lines_to_next_cell": 2,
        "outputId": "5b0ac708-b6ab-473c-d1f6-8ba99c0ff66c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-01 13:57:39--  https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\n",
            "Resolving download.microsoft.com (download.microsoft.com)... 104.80.224.107, 2a02:26f0:6d00:688::e59, 2a02:26f0:6d00:6bf::e59\n",
            "Connecting to download.microsoft.com (download.microsoft.com)|104.80.224.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 824894548 (787M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/cats-and-dogs.zip’\n",
            "\n",
            "/tmp/cats-and-dogs. 100%[===================>] 786.68M   141MB/s    in 5.5s    \n",
            "\n",
            "2022-04-01 13:57:45 (143 MB/s) - ‘/tmp/cats-and-dogs.zip’ saved [824894548/824894548]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# If the URL doesn't work, visit https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\n",
        "# And right click on the 'Download Manually' link to get a new URL to the dataset\n",
        "\n",
        "# Note: This is a very large dataset and will take some time to download\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n",
        "    -O \"/tmp/cats-and-dogs.zip\"\n",
        "\n",
        "local_zip = '/tmp/cats-and-dogs.zip'\n",
        "zip_ref   = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "id": "3sd9dQWa23aj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_HsUV9WVJHL"
      },
      "source": [
        "Now the images are stored within the `/tmp/PetImages` directory. There is a subdirectory for each class, so one for dogs and one for cats."
      ],
      "id": "e_HsUV9WVJHL"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DM851ZmN28J3",
        "outputId": "a25856ce-3a54-47aa-89dd-f55f9a35d9ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 12501 images of dogs.\n",
            "There are 12501 images of cats.\n"
          ]
        }
      ],
      "source": [
        "source_path = '/tmp/PetImages'\n",
        "\n",
        "source_path_dogs = os.path.join(source_path, 'Dog')\n",
        "source_path_cats = os.path.join(source_path, 'Cat')\n",
        "\n",
        "\n",
        "# os.listdir returns a list containing all files under the given path\n",
        "print(f\"There are {len(os.listdir(source_path_dogs))} images of dogs.\")\n",
        "print(f\"There are {len(os.listdir(source_path_cats))} images of cats.\")"
      ],
      "id": "DM851ZmN28J3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7dI86rmRGmC"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "There are 12501 images of dogs.\n",
        "There are 12501 images of cats.\n",
        "```"
      ],
      "id": "G7dI86rmRGmC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFbMliudNIjW"
      },
      "source": [
        "You will need a directory for cats-v-dogs, and subdirectories for training\n",
        "and testing. These in turn will need subdirectories for 'cats' and 'dogs'. To accomplish this, complete the `create_train_test_dirs` below:"
      ],
      "id": "iFbMliudNIjW"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "code",
        "id": "F-QkLjxpmyK2",
        "outputId": "4c019e87-6037-45ef-a4ca-e348ef05bb56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating '/tmp/cats-v-dogs' directory.\n",
            "Training directory created as '/tmp/cats-v-dogs/training'.\n",
            "Testing directory created as '/tmp/cats-v-dogs/testing'.\n",
            "Creating the subfolders for storing cats and dogs images.\n",
            "'/tmp/cats-v-dogs/training/cats' and '/tmp/cats-v-dogs/training/dogs' created.\n",
            "'/tmp/cats-v-dogs/testing/cats' and '/tmp/cats-v-dogs/testing/dogs' created.\n"
          ]
        }
      ],
      "source": [
        "# Define root directory\n",
        "root_dir = '/tmp/cats-v-dogs'\n",
        "\n",
        "# Empty directory to prevent FileExistsError is the function is run several times\n",
        "if os.path.exists(root_dir):\n",
        "  shutil.rmtree(root_dir)\n",
        "\n",
        "# GRADED FUNCTION: create_train_test_dirs\n",
        "def create_train_test_dirs(root_path):\n",
        "  ### START CODE HERE\n",
        "  #For this exercise, we must keep as arguments only the variables already defined for the graded functions.\n",
        "\n",
        "  # HINT:\n",
        "  # Use os.makedirs to create your directories with intermediate subdirectories\n",
        "  # Don't hardcode the paths. Use os.path.join to append the new directories to the root_path parameter\n",
        "\n",
        "  print(f\"Creating \\'{root_path}\\' directory.\")\n",
        "  #Slash makes the code ignore the following character. This allows us to insert \n",
        "  #quotes inside a string.\n",
        "\n",
        "  #Use makedirs method to create the directory:\n",
        "  os.makedirs(root_path)\n",
        "\n",
        "  #Create directory of the images used for training:\n",
        "  #Define subdirectory path:\n",
        "  training_dir = os.path.join(root_path, 'training')\n",
        "  \n",
        "  #Create subdirectory on the defined path:\n",
        "  os.makedirs(training_dir)\n",
        "  print(f\"Training directory created as \\'{training_dir}\\'.\")\n",
        "\n",
        "  #Create directory of the images used for testing:\n",
        "  #Define subdirectory path:\n",
        "  testing_dir = os.path.join(root_path, 'testing')\n",
        "  \n",
        "  #Create subdirectory on the defined path:\n",
        "  os.makedirs(testing_dir)\n",
        "  print(f\"Testing directory created as \\'{testing_dir}\\'.\")\n",
        "\n",
        "  #Create subfolders that will store the cats and dogs images\n",
        "  print(\"Creating the subfolders for storing cats and dogs images.\")\n",
        "  # Directory with training cat/dog pictures\n",
        "  #Define subdirectories paths:\n",
        "  training_cats_dir = os.path.join(training_dir, 'cats')\n",
        "  training_dogs_dir = os.path.join(training_dir, 'dogs')\n",
        "  \n",
        "  #Create subdirectories on the defined paths:\n",
        "  os.makedirs(training_cats_dir)\n",
        "  os.makedirs(training_dogs_dir)\n",
        "  print(f\"\\'{training_cats_dir}\\' and \\'{training_dogs_dir}\\' created.\")\n",
        "\n",
        "  # Directory with testing cat/dog pictures\n",
        "  #Define subdirectories paths:\n",
        "  testing_cats_dir = os.path.join(testing_dir, 'cats')\n",
        "  testing_dogs_dir = os.path.join(testing_dir, 'dogs')\n",
        "  \n",
        "  #Create subdirectories on the defined paths:\n",
        "  os.makedirs(testing_cats_dir)\n",
        "  os.makedirs(testing_dogs_dir)\n",
        "  print(f\"\\'{testing_cats_dir}\\' and \\'{testing_dogs_dir}\\' created.\")\n",
        "\n",
        "  pass\n",
        "\n",
        "  ### END CODE HERE\n",
        "\n",
        "try:\n",
        "  create_train_test_dirs(root_path=root_dir)\n",
        "except FileExistsError:\n",
        "  print(\"You should not be seeing this since the upper directory is removed beforehand\")"
      ],
      "id": "F-QkLjxpmyK2"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5dhtL344OK00",
        "outputId": "ee0b9940-9e6e-4df0-90c4-ac55c790d4b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/cats-v-dogs/training\n",
            "/tmp/cats-v-dogs/testing\n",
            "/tmp/cats-v-dogs/training/cats\n",
            "/tmp/cats-v-dogs/training/dogs\n",
            "/tmp/cats-v-dogs/testing/cats\n",
            "/tmp/cats-v-dogs/testing/dogs\n"
          ]
        }
      ],
      "source": [
        "# Test your create_train_test_dirs function\n",
        "\n",
        "for rootdir, dirs, files in os.walk(root_dir):\n",
        "    for subdir in dirs:\n",
        "        print(os.path.join(rootdir, subdir))"
      ],
      "id": "5dhtL344OK00"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7A0RK3IQsvg"
      },
      "source": [
        "**Expected Output (directory order might vary):**\n",
        "\n",
        "``` txt\n",
        "/tmp/cats-v-dogs/training\n",
        "/tmp/cats-v-dogs/testing\n",
        "/tmp/cats-v-dogs/training/cats\n",
        "/tmp/cats-v-dogs/training/dogs\n",
        "/tmp/cats-v-dogs/testing/cats\n",
        "/tmp/cats-v-dogs/testing/dogs\n",
        "\n",
        "```"
      ],
      "id": "D7A0RK3IQsvg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R93T7HdE5txZ"
      },
      "source": [
        "Code the `split_data` function which takes in the following arguments:\n",
        "- SOURCE: directory containing the files\n",
        "\n",
        "- TRAINING: directory that a portion of the files will be copied to (will be used for training)\n",
        "- TESTING: directory that a portion of the files will be copied to (will be used for testing)\n",
        "- SPLIT SIZE: to determine the portion\n",
        "\n",
        "The files should be randomized, so that the training set is a random sample of the files, and the test set is made up of the remaining files.\n",
        "\n",
        "For example, if `SOURCE` is `PetImages/Cat`, and `SPLIT` SIZE is .9 then 90% of the images in `PetImages/Cat` will be copied to the `TRAINING` dir\n",
        "and 10% of the images will be copied to the `TESTING` dir.\n",
        "\n",
        "All images should be checked before the copy, so if they have a zero file length, they will be omitted from the copying process. If this is the case then your function should print out a message such as `\"filename is zero length, so ignoring.\"`. **You should perform this check before the split so that only non-zero images are considered when doing the actual split.**\n",
        "\n",
        "\n",
        "Hints:\n",
        "\n",
        "- `os.listdir(DIRECTORY)` returns a list with the contents of that directory.\n",
        "\n",
        "- `os.path.getsize(PATH)` returns the size of the file\n",
        "\n",
        "- `copyfile(source, destination)` copies a file from source to destination\n",
        "\n",
        "- `random.sample(list, len(list))` shuffles a list"
      ],
      "id": "R93T7HdE5txZ"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "code",
        "id": "zvSODo0f9LaU"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: split_data\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "  \n",
        "  import numpy as np\n",
        "\n",
        "  ### START CODE HERE\n",
        "  #SOURCE = Original directory containing the files\n",
        "  #TRAINING = Directory that will receive the files for training the neural networks.\n",
        "  #TESTING = Directory that will receive the files for testing the neural networks.\n",
        "  #SPLIT_SIZE = Percent of split between the files - fraction destined to\n",
        "  #training\n",
        "  #This function should be analogous to Sklearn's train_test_split.\n",
        "\n",
        "  #If SOURCE == root directory, len(SOURCE) == 2, since this directory contains\n",
        "  #only two items, the two sub-directories.\n",
        "  #Therefore, SORCE must be the path of the subdirectory itself:\n",
        "  # SOURCE = '/tmp/PetImages/Dog' for the dog images\n",
        "  # SOURCE = '/tmp/PetImages/Cat' for the cat images\n",
        "\n",
        "  #Get a list containing the contents of the SOURCE directory:\n",
        "  files_list = os.listdir(SOURCE)\n",
        "\n",
        "  #Each element of this list is the NAME of the file, not its full path.\n",
        "  #For instance, one element could be \"3447.jpg\". Notice that the full path\n",
        "  #of the file is the concatenation between the SOURCE (\"/tmp/PetImages/Dog\"), \n",
        "  # \"/\", and the file name.\n",
        "  \n",
        "  #Get the total of files:\n",
        "  total_files = len(files_list)\n",
        "\n",
        "  #Get total of files used for training:\n",
        "  total_training_files = int(np.rint(SPLIT_SIZE * total_files))\n",
        "  #We must use the int function to guarantee that the list will contain an\n",
        "  #integer number of files (we cannot have a fraction of a file).\n",
        "  #The int function guarantees that the variable will be stored as an integer.\n",
        "  #The numpy.rint(a) function rounds elements of the array to the nearest integer.\n",
        "  #https://numpy.org/doc/stable/reference/generated/numpy.rint.html\n",
        "  #For values exactly halfway between rounded decimal values, \n",
        "  #NumPy rounds to the nearest even value. \n",
        "  #Thus 1.5 and 2.5 round to 2.0, -0.5 and 0.5 round to 0.0, etc.\n",
        "\n",
        "  #Get total of files used for testing:\n",
        "  total_testing_files = total_files - total_training_files\n",
        "\n",
        "  #Function random.sample(input_list, number_of_samples): this function creates a list containing\n",
        "  #a total of elements equals to the parameter \"number_of_samples\", which must be an integer.\n",
        "  #This list is obtained by ramdomly selecting a total of \"number_of_samples\" elements from the\n",
        "  #list \"input_list\" passed as parameter.\n",
        "\n",
        "  #Create a list of the files used for training, by sampling the files_list:\n",
        "  training_files = random.sample(files_list, total_training_files)\n",
        "\n",
        "  #Create a list of the files used for testing.\n",
        "  #1) Create an empty list:\n",
        "  testing_files = []\n",
        "\n",
        "  #2) Loop through the original files_list and check if the file is in the training list, \n",
        "  #by using the \"in\" method. If it is not, then append it to the testing_files list.\n",
        "  #Notice that the Pandas \"isin\" method is the analogous, but applies to dataframes, not\n",
        "  #to lists.\n",
        "  \n",
        "  #Loop through each element (named 'file') of the original files_list:\n",
        "  for file in files_list:\n",
        "\n",
        "      #create a boolean checker, that can be either True or False:\n",
        "      bool_check = file in training_files\n",
        "      #If file is one item from 'training_files' list, the bool_check = True.\n",
        "      #If it is not, bool_check = False\n",
        "\n",
        "      if (bool_check == False):\n",
        "        #This file is not on training_files list, so we can copy it to the \n",
        "        #testing_files list, by appending it:\n",
        "        testing_files.append(file)\n",
        "\n",
        "  #Now, we can copy the training_list to the training directories of cats and dogs.\n",
        "  #Before copying, we must check if the file is not empty, and do not copy the empty\n",
        "  #ones.\n",
        "\n",
        "  #Notice that the SOURCE file is divided by several slashs (\"/\").\n",
        "  #We can apply the Python default method str.split(sep=None, maxsplit=- 1)\n",
        "  #where sep is the character used for separation (whitespace is the default separator); \n",
        "  #and maxsplit is the maximum number of splits allowed. If maxsplit = -1 or if it is\n",
        "  #not specified, there is no maximum of splits.\n",
        "  #https://docs.python.org/dev/library/stdtypes.html#str.split\n",
        "  #https://numpy.org/doc/stable/reference/routines.char.html\n",
        "  \n",
        "  #The str.split method is from the default library of Python, and must be applied\n",
        "  #to objects of type string. It returns a list of substrings.\n",
        "\n",
        "  #If we set sep = \"/\", the str.split method will return a list constituted by\n",
        "  # several substrings separated by the slash. For example. If we separate '/tmp/PetImages/Dog', \n",
        "  # the function will return ['', 'tmp', 'PetImages', 'Dog'].\n",
        "  #The last element of the list will be either \"Dog\" or \"Cat\", and will define where we should save\n",
        "  #the files, in the cat or in the dog directory.\n",
        "\n",
        "  #Create the list of substrings\n",
        "  list_of_substrings = SOURCE.split(sep = \"/\")\n",
        "  #Take the last element (file_class) of this list. The total of elements is:\n",
        "  # len(list_of_substrings). Since indexing starts from zero, the last element\n",
        "  # is the one with index = len(list_of_substrings) - 1\n",
        "  file_class = list_of_substrings[(len(list_of_substrings) - 1)]\n",
        "\n",
        "  #file_class = 'Dog' or file_class = 'Cat'\n",
        "\n",
        "  #Now, let's copy the files on the training directory.\n",
        "\n",
        "  #Loop through each element (file) of the training_files list:\n",
        "  for file in training_files:\n",
        "\n",
        "    #Retrieve the full file path, by joining the file name to the SOURCE path:\n",
        "    file_path = os.path.join(SOURCE, file)\n",
        "    #Get the file size:\n",
        "    file_size = os.path.getsize(file_path)\n",
        "\n",
        "    #Check if the size is higher than zero (non-corrupted images):\n",
        "    if (file_size > 0):\n",
        "\n",
        "        #Now, we can copythe non-corrupted file from file_path to the new directory.\n",
        "        #The training directory was passed as the parameter TRAINING\n",
        "        #copyfile(source, destination) copies a file from source to destination\n",
        "        #New file path:\n",
        "        new_path = os.path.join(TRAINING, file)\n",
        "        copyfile(file_path, new_path)\n",
        "    \n",
        "    else: \n",
        "      #print the message for the zero-sized file\n",
        "      printed_msg = file + \" is zero length, so ignoring.\"\n",
        "      print(printed_msg)\n",
        "    \n",
        "    #Now that we copied all the files to the training directories, we can repeat\n",
        "    #the proces s for the testing directory:\n",
        "\n",
        "  #Loop through each element (file) of the testing_files list:\n",
        "  for file in testing_files:\n",
        "\n",
        "    #Retrieve the full file path, by joining the file name to the SOURCE path:\n",
        "    file_path = os.path.join(SOURCE, file)\n",
        "    #Get the file size:\n",
        "    file_size = os.path.getsize(file_path)\n",
        "\n",
        "    #Check if the size is higher than zero (non-corrupted images):\n",
        "    if (file_size > 0):\n",
        "\n",
        "        #Now, we can copythe non-corrupted file from file_path to the new directory.\n",
        "        #The testing directory was passed as the parameter TESTING\n",
        "        #New file path:\n",
        "        new_path = os.path.join(TESTING, file)\n",
        "        copyfile(file_path, new_path)\n",
        "       \n",
        "    else: \n",
        "      #print the message for the zero-sized file\n",
        "      printed_msg = file + \" is zero length, so ignoring.\"\n",
        "      print(printed_msg)\n",
        "\n",
        "  pass\n",
        "\n",
        "  ### END CODE HERE"
      ],
      "id": "zvSODo0f9LaU"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FlIdoUeX9S-9",
        "outputId": "cbe2d2e3-7ba0-43fa-c803-56e30620f331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "666.jpg is zero length, so ignoring.\n",
            "11702.jpg is zero length, so ignoring.\n",
            "\n",
            "\n",
            "There are 11251 images of cats for training\n",
            "There are 11250 images of dogs for training\n",
            "There are 1249 images of cats for testing\n",
            "There are 1250 images of dogs for testing\n"
          ]
        }
      ],
      "source": [
        "# Test your split_data function\n",
        "\n",
        "# Define paths\n",
        "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
        "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
        "\n",
        "TRAINING_DIR = \"/tmp/cats-v-dogs/training/\"\n",
        "TESTING_DIR = \"/tmp/cats-v-dogs/testing/\"\n",
        "\n",
        "TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, \"cats/\")\n",
        "TESTING_CATS_DIR = os.path.join(TESTING_DIR, \"cats/\")\n",
        "\n",
        "TRAINING_DOGS_DIR = os.path.join(TRAINING_DIR, \"dogs/\")\n",
        "TESTING_DOGS_DIR = os.path.join(TESTING_DIR, \"dogs/\")\n",
        "\n",
        "# Empty directories in case you run this cell multiple times\n",
        "if len(os.listdir(TRAINING_CATS_DIR)) > 0:\n",
        "  for file in os.scandir(TRAINING_CATS_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TRAINING_DOGS_DIR)) > 0:\n",
        "  for file in os.scandir(TRAINING_DOGS_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TESTING_CATS_DIR)) > 0:\n",
        "  for file in os.scandir(TESTING_CATS_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TESTING_DOGS_DIR)) > 0:\n",
        "  for file in os.scandir(TESTING_DOGS_DIR):\n",
        "    os.remove(file.path)\n",
        "\n",
        "# Define proportion of images used for training\n",
        "split_size = .9\n",
        "\n",
        "# Run the function\n",
        "# NOTE: Messages about zero length images should be printed out\n",
        "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
        "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n",
        "\n",
        "# Check that the number of images matches the expected output\n",
        "print(f\"\\n\\nThere are {len(os.listdir(TRAINING_CATS_DIR))} images of cats for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_DOGS_DIR))} images of dogs for training\")\n",
        "print(f\"There are {len(os.listdir(TESTING_CATS_DIR))} images of cats for testing\")\n",
        "print(f\"There are {len(os.listdir(TESTING_DOGS_DIR))} images of dogs for testing\")"
      ],
      "id": "FlIdoUeX9S-9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvskJNOFVSaz"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "666.jpg is zero length, so ignoring.\n",
        "11702.jpg is zero length, so ignoring.\n",
        "```\n",
        "\n",
        "```\n",
        "There are 11250 images of cats for training\n",
        "There are 11250 images of dogs for training\n",
        "There are 1250 images of cats for testing\n",
        "There are 1250 images of dogs for testing\n",
        "```"
      ],
      "id": "hvskJNOFVSaz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zil4QmOD_mXF"
      },
      "source": [
        "Now that you have successfully organized the data in a way that can be easily fed to Keras' `ImageDataGenerator`, it is time for you to code the generators that will yield batches of images, both for training and validation. For this, complete the `train_val_generators` function below.\n",
        "\n",
        "Something important to note is that the images in this dataset come in a variety of resolutions. Luckily, the `flow_from_directory` method allows you to standarize this by defining a tuple called `target_size` that will be used to convert each image to this target resolution. **For this exercise, use a `target_size` of (150, 150)**.\n",
        "\n",
        "**Note:** So far, you have seen the term `testing` being used a lot for referring to a subset of images within the dataset. In this exercise, all of the `testing` data is actually being used as `validation` data. This is not very important within the context of the task at hand but it is worth mentioning to avoid confusion."
      ],
      "id": "Zil4QmOD_mXF"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "code",
        "id": "fQrZfVgz4j2g"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: train_val_generators\n",
        "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
        "  \n",
        "  ### START CODE HERE\n",
        "  from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
        "  #Pixel intensity goes from 0 to 255. Let's normalize it so that it goes from 0 to 1:\n",
        "  train_datagen = ImageDataGenerator(rescale = (1.0)/(255.))\n",
        "\n",
        "  # Pass in the appropiate arguments to the flow_from_directory method\n",
        "  train_generator = train_datagen.flow_from_directory(directory = TRAINING_DIR,\n",
        "                                                      batch_size = 200,\n",
        "                                                      class_mode = \"binary\",\n",
        "                                                      target_size = (150, 150))\n",
        "  #batch_size represents the amount of data used in each training cycle. Since we\n",
        "  #have 22500 images for training, we will have 120 batches using 200 images.\n",
        "  #Other class modes are 'categorical' or 'sparse'. \n",
        "  #The image target_size should be (150, 150) as mentioned earlier\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
        "  validation_datagen = ImageDataGenerator(rescale = (1.0)/(255.))\n",
        "\n",
        "  # Pass in the appropiate arguments to the flow_from_directory method\n",
        "  validation_generator = validation_datagen.flow_from_directory(directory = VALIDATION_DIR,\n",
        "                                                                batch_size = 20,\n",
        "                                                                class_mode = \"binary\",\n",
        "                                                                target_size = (150, 150))\n",
        "  #For testing, we have 2500 images, so we will use 120 batches of 20 images\n",
        "  ### END CODE HERE\n",
        "  return train_generator, validation_generator"
      ],
      "id": "fQrZfVgz4j2g"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qM7FxrjGiobD",
        "outputId": "86de1654-f6f4-4afe-c708-9a1bea49b421",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22499 images belonging to 2 classes.\n",
            "Found 2499 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Test your generators\n",
        "train_generator, validation_generator = train_val_generators(TRAINING_DIR, TESTING_DIR)"
      ],
      "id": "qM7FxrjGiobD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiPNmSfZjHwJ"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "Found 22498 images belonging to 2 classes.\n",
        "Found 2500 images belonging to 2 classes.\n",
        "```\n"
      ],
      "id": "tiPNmSfZjHwJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI3oEmyQCZoO"
      },
      "source": [
        "One last step before training is to define the architecture of the model that will be trained.\n",
        "\n",
        "Complete the `create_model` function below which should return a Keras' `Sequential` model.\n",
        "\n",
        "Aside from defining the architecture of the model, you should also compile it so make sure to use a `loss` function that is compatible with the `class_mode` you defined in the previous exercise, which should also be compatible with the output of your network. You can tell if they aren't compatible if you get an error during training.\n",
        "\n",
        "**Note that you should use at least 3 convolution layers to achieve the desired performance.**"
      ],
      "id": "TI3oEmyQCZoO"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "code",
        "id": "oDPK8tUB_O9e",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: create_model\n",
        "def create_model():\n",
        "\n",
        "  # DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n",
        "  # USE AT LEAST 3 CONVOLUTION LAYERS\n",
        "\n",
        "  ### START CODE HERE\n",
        "\n",
        "  model = tf.keras.models.Sequential([ \n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color (R, G, B)\n",
        "    \n",
        "    #First convolution:\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    #The (3, 3) kernel reduces the image in 2 pixels for axis X; and 2 pixels for axis Y.\n",
        "    #Now, the image is 148 x 148\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    #The (2,2) max pooling divides by 2 the length of each axis.\n",
        "    #Now, the image is 74 x 74 pixels\n",
        "\n",
        "    #Second convolution:\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    #Image gets 72 x 72\n",
        "    tf.keras.layers.MaxPooling2D(2,2), \n",
        "    #Image gets 36 x 36\n",
        "\n",
        "    #Third convolution:\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
        "    #Image is now 34 x 34\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    #Image was reduced to 17 x 17.\n",
        "\n",
        "    #Fourth convolution:\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'), \n",
        "    #Image is now 15 x 15\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    #Image was reduced to 7 x 7 (rounded down)\n",
        "\n",
        "    #If we applied a 5th convolution, the images would get excessively\n",
        "    #reduced to only 2x2 pixels after pooling. Then, there would be to few\n",
        "    #information retrieved. Also, the adding of convolutional layers can\n",
        "    #increase the training time too much. The 4 ConvNets showed to be the best\n",
        "    #architecture for this case, allowing the achievement of the desired accuracy.\n",
        "\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'), \n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') \n",
        "    #and 1 for the other ('dogs')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  \n",
        "  ])\n",
        "  \n",
        "  #The best optimizer for this model is 'adam' which automatically adjusts the\n",
        "  #learning rates, i.e., the rate of correction of the weights.\n",
        "  #In the beginning, a too high learning rate leads to very intense errors and\n",
        "  #difficulty on finding the optimum. So, in the beginning, the learning rate \n",
        "  #should be low until the model finds the best direction for adjusting its parameters.\n",
        "  #On the other hand, at the end of the process, the learning rate should be increased.\n",
        "  #If it is not, the model will adjust its weights in a very low rate, and will show very\n",
        "  #few improvement from one epoch to the other.\n",
        "  #RMSProp does not allow us to adjust the learning rate during training, but 'adam' does\n",
        "  #that for us.\n",
        "  model.compile(optimizer = 'adam',\n",
        "                loss = 'binary_crossentropy',\n",
        "                metrics=['accuracy']) \n",
        "    \n",
        "  ### END CODE HERE\n",
        "\n",
        "  return model\n"
      ],
      "id": "oDPK8tUB_O9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMFNJZmTCZv6"
      },
      "source": [
        "Now it is time to train your model!\n",
        "\n",
        "**Note:** You can ignore the `UserWarning: Possibly corrupt EXIF data.` warnings."
      ],
      "id": "SMFNJZmTCZv6"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5qE1G6JB4fMn",
        "outputId": "30fec96b-83b2-4bd1-9834-74da497f810b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "  8/110 [=>............................] - ETA: 59s - loss: 0.7229 - accuracy: 0.4950"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "110/110 [==============================] - 91s 723ms/step - loss: 0.6566 - accuracy: 0.5945 - val_loss: 0.5967 - val_accuracy: 0.6735\n",
            "Epoch 2/15\n",
            "110/110 [==============================] - 78s 710ms/step - loss: 0.5533 - accuracy: 0.7157 - val_loss: 0.4807 - val_accuracy: 0.7770\n",
            "Epoch 3/15\n",
            "110/110 [==============================] - 77s 703ms/step - loss: 0.4725 - accuracy: 0.7755 - val_loss: 0.4691 - val_accuracy: 0.7760\n",
            "Epoch 4/15\n",
            "110/110 [==============================] - 77s 699ms/step - loss: 0.4192 - accuracy: 0.8081 - val_loss: 0.3907 - val_accuracy: 0.8220\n",
            "Epoch 5/15\n",
            "110/110 [==============================] - 77s 699ms/step - loss: 0.3769 - accuracy: 0.8311 - val_loss: 0.3921 - val_accuracy: 0.8275\n",
            "Epoch 6/15\n",
            "110/110 [==============================] - 76s 689ms/step - loss: 0.3396 - accuracy: 0.8541 - val_loss: 0.3770 - val_accuracy: 0.8340\n",
            "Epoch 7/15\n",
            "110/110 [==============================] - 76s 689ms/step - loss: 0.3143 - accuracy: 0.8623 - val_loss: 0.3333 - val_accuracy: 0.8525\n",
            "Epoch 8/15\n",
            "110/110 [==============================] - 77s 695ms/step - loss: 0.2797 - accuracy: 0.8821 - val_loss: 0.3072 - val_accuracy: 0.8685\n",
            "Epoch 9/15\n",
            "110/110 [==============================] - 79s 717ms/step - loss: 0.2619 - accuracy: 0.8908 - val_loss: 0.3307 - val_accuracy: 0.8605\n",
            "Epoch 10/15\n",
            "110/110 [==============================] - 80s 724ms/step - loss: 0.2299 - accuracy: 0.9045 - val_loss: 0.2968 - val_accuracy: 0.8675\n",
            "Epoch 11/15\n",
            "110/110 [==============================] - 79s 715ms/step - loss: 0.2097 - accuracy: 0.9146 - val_loss: 0.3388 - val_accuracy: 0.8590\n",
            "Epoch 12/15\n",
            "110/110 [==============================] - 75s 678ms/step - loss: 0.1874 - accuracy: 0.9223 - val_loss: 0.3802 - val_accuracy: 0.8410\n",
            "Epoch 13/15\n",
            "110/110 [==============================] - 78s 712ms/step - loss: 0.1666 - accuracy: 0.9314 - val_loss: 0.3515 - val_accuracy: 0.8575\n",
            "Epoch 14/15\n",
            "110/110 [==============================] - 75s 678ms/step - loss: 0.1423 - accuracy: 0.9441 - val_loss: 0.3466 - val_accuracy: 0.8765\n",
            "Epoch 15/15\n",
            "110/110 [==============================] - 79s 722ms/step - loss: 0.1242 - accuracy: 0.9516 - val_loss: 0.3212 - val_accuracy: 0.8850\n"
          ]
        }
      ],
      "source": [
        "# Get the untrained model\n",
        "model = create_model()\n",
        "\n",
        "# Train the model\n",
        "# Note that this may take some time.\n",
        "history = model.fit(train_generator,\n",
        "                    #In this exercise, we are not allowed to increase the number of epochs.\n",
        "                    #On the other hand, increasing 'steps_per_epoch will increase the learning\n",
        "                    #period (the duration) of each epoch, presenting a similar\n",
        "                    #effect. So, one strategy is to tune the hyperparameter\n",
        "                    #steps_per_epoch instead of changing the number of epochs.\n",
        "                    steps_per_epoch=110,\n",
        "                    epochs=15,\n",
        "                    validation_steps=100,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_generator)"
      ],
      "id": "5qE1G6JB4fMn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGsaDMc-GMd4"
      },
      "source": [
        "Once training has finished, you can run the following cell to check the training and validation accuracy achieved at the end of each epoch.\n",
        "\n",
        "**To pass this assignment, your model should achieve a training accuracy of at least 95% and a validation accuracy of at least 80%**. If your model didn't achieve these thresholds, try training again with a different model architecture and remember to use at least 3 convolutional layers."
      ],
      "id": "VGsaDMc-GMd4"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MWZrJN4-65RC",
        "outputId": "0e95528a-02e5-4019-800a-ebe8ea3a4dd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.show()\n",
        "print(\"\")\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
        "plt.show()"
      ],
      "id": "MWZrJN4-65RC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYIaqsN2pav6"
      },
      "source": [
        "You will probably encounter that the model is overfitting, which means that it is doing a great job at classifying the images in the training set but struggles with new data. This is perfectly fine and you will learn how to mitigate this issue in the upcoming week.\n",
        "\n",
        "Before downloading this notebook and closing the assignment, be sure to also download the `history.pkl` file which contains the information of the training history of your model. You can download this file by running the cell below:"
      ],
      "id": "NYIaqsN2pav6"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yWcrc9nZTsHj",
        "outputId": "8681a754-3649-4d0b-e702-59b59e0a046d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9b5361d5-903f-4656-97d0-149cadaa9063\", \"history.pkl\", 628)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def download_history():\n",
        "  import pickle\n",
        "  from google.colab import files\n",
        "\n",
        "  with open('history.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "  files.download('history.pkl')\n",
        "\n",
        "download_history()"
      ],
      "id": "yWcrc9nZTsHj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF4RihujZ_F4"
      },
      "source": [
        "You will also need to submit this notebook for grading. To download it, click on the `File` tab in the upper left corner of the screen then click on `Download` -> `Download .ipynb`. You can name it anything you want as long as it is a valid `.ipynb` (jupyter notebook) file."
      ],
      "id": "OF4RihujZ_F4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joAaZSWWpbOI"
      },
      "source": [
        "**Congratulations on finishing this week's assignment!**\n",
        "\n",
        "You have successfully implemented a convolutional neural network that classifies images of cats and dogs, along with the helper functions needed to pre-process the images!\n",
        "\n",
        "**Keep it up!**"
      ],
      "id": "joAaZSWWpbOI"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "C2W1_Assignment.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}